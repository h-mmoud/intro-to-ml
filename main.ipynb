{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workshop Demo**: Introduction to Machine Learning in Finance\n",
    "\n",
    "- **Objectives**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Price Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hassan TODO\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Risk Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nico TODO\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning in Option Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Deep Learning**\n",
    "\n",
    "- [Deep Learning](https://www.ibm.com/topics/deep-learning) is a subset of machine learning that uses multilayered neural networks to simulate the complex decision-making power of the human brain.\n",
    "\n",
    "- Main difference between deep learning and machine learning is the structure of the underlying neural network architecture. \"Nondeep\" traditional ML models use simple neural networks with one or two computational layers. Deep learning models use neural networks with one or two compuetational layers. Deep learning models use three or more layers (typically hundreds or thousands of layers) to train the models.\n",
    "\n",
    "- Consists of multiple layers of interconnected notes, each building on previous to refine and optimize the prediction -> forward propogation. The `IN` and `OUT` layers of a deep neural network are called **visible** layers\n",
    "\n",
    "- Another process, backward propagation, is used to calculate errors in predictions and then adjust the weights and biases of between the node layers accordingly. The common approach to do this is via `gradient descent`.\n",
    "\n",
    "- Common deep learning neural networks: \n",
    "    - **CNNs** (convolution neural networks): computer vision and image classification applications.\n",
    "    - **RNNs** (recurrent neural networks): natural language and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Option Pricing**\n",
    "- Formal definition of an [option](https://corporatefinanceinstitute.com/resources/derivatives/option-pricing-models/) states that it is a type of contract between two parties that provides one party the right, but **not** the obligation, to buy or sell the underlying asset at a predetermined price before or at expiration day.\n",
    "\n",
    "    - Call option = buying\n",
    "    - Put option = selling\n",
    "\n",
    "- Buying or selling an option comes with a price called the option's premium. Buyer of an option pay the premium and sellers receive the premium\n",
    "\n",
    "- Some common factors determining the value of an option:\n",
    "\n",
    "    - [Intrinsic value](https://www.investopedia.com/terms/i/intrinsicvalue.asp)\n",
    "    - Time to expiration\n",
    "    - Volatility\n",
    "    - Interest rates\n",
    "\n",
    "(ie. price of a stock rises -> likely price of call option will rise and the price of a put option will fall, vice versa)\n",
    "\n",
    "- Example pricing models: \n",
    "\n",
    "    - Binomial Model\n",
    "    - Trinomial Model\n",
    "    - Black-Scholes Model\n",
    "    - Monte-Carlo Simulation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Black-Scholes\n",
    "\n",
    "- One of the most commonly used formula to price call and put options, there are many variations of this formula.\n",
    "- Model was first discovered in 1973 by economists Fischer black and Myron Scholes, developed mainly for pricing European options on stocks.\n",
    "\n",
    "- Some assumptions:\n",
    "    - Stock Price Distribution : continuously compounded returns on the stock are normally distributed and independent over time, volatility of continuously compounded returns is known and constant, future dividends are known (as a dollar amount/fixed dividend yield)\n",
    "    - Economic Environment : risk-free rate is known and constant, no transaction costs or taxes, possible to short-sell with no cost and to borrow at the risk-free rate\n",
    "\n",
    "- An example of deep learning model using Black-Scholes to predict option pricing: [Sanskar](https://github.com/Sanskar02/OPTION_PRICING_MODEL)\n",
    "- Another example model can be found [here](https://srdas.github.io/Papers/BlackScholesNN.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Deep Learning Model in Option Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load and prepare dataset**\n",
    "\n",
    "- Historical option data from [AlphaVantage](https://www.alphavantage.co/)\n",
    "- Measures and variables chosen based off factors of Black-Scholes model\n",
    "- Extracting data for call options only\n",
    "- Randomly generating interest rate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'C:/Users/ziyin/OneDrive/Desktop/intro-to-ml/historical_options_IBM.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Convert 'expiration' and 'date' columns to datetime format\n",
    "data['expiration'] = pd.to_datetime(data['expiration'])\n",
    "data['current'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Calculate time to expiration (T) in years\n",
    "today = datetime.now().date()\n",
    "data['T'] = (data['expiration'].dt.date - data['current'].dt.date).apply(lambda x: x.days) / 365\n",
    "\n",
    "filtered_data = data[data['type']=='call']\n",
    "\n",
    "# Map relevant columns to the desired format, and add placeholders if necessary\n",
    "df_formatted = filtered_data.rename(columns={\n",
    "    'last': 'S',                # Use 'last' as a placeholder for spot price if applicable\n",
    "    'strike': 'K',\n",
    "    'implied_volatility': 'sigma'\n",
    "})\n",
    "\n",
    "# Add placeholders and 'r' (risk-free rate)\n",
    "df_formatted['r'] = 0.01  # Placeholder for a standard risk-free rate\n",
    "\n",
    "\n",
    "# Create a new DataFrame that only contains the desired columns\n",
    "df = df_formatted[['S', 'K', 'T', 'r', 'sigma']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving call values from spreadsheet\n",
    "df['C'] = filtered_data['bid']\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new DataFrame object df1 that points to same objects as df\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the roperty of linear homogeneity in the Black-Scholes model\n",
    "# Model's output price scales linearly with scale of input\n",
    "df1[\"S\"] = df1[\"S\"]/df1[\"K\"]\n",
    "df1[\"C\"] = df1[\"C\"]/df1[\"K\"]\n",
    "\n",
    "# Now database S represents ratio of spot price to strike price (S/K)\n",
    "# C represents ratio of normalizes option price (C/K)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing column K as it is now implied in the dataset under columns S and C\n",
    "df1.drop(columns = ['K'], inplace = True)\n",
    "df1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.copy()\n",
    "# Dropping columns C and sigma (option price and volatility) to initialse training dataset\n",
    "X.drop(columns = ['C'], inplace = True)\n",
    "X.drop(columns = ['sigma'], inplace = True)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining y as normalized C (option price) from before for future reference\n",
    "y = df1['C']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creation of synthetic datasets and for splitting arrays/DataFrames into random train and test subsets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits feature matrix X and target vector y into training and testing sets\n",
    "# 80% of data will be used for training and remaining 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 1)\n",
    "\n",
    "# Now stores 80% of X dataframe\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores 80% of y dataframe, ground truth labels that the model will learn to predict from data in X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://www.tensorflow.org/) framework is used for creating and deploying ML models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing TensorFlow library to prepare custom activation functions\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Defining a custom activation function\n",
    "def custom_activation(x):\n",
    "    return K.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the ANN(artificial neural network) Model**\n",
    "\n",
    "Three activation layers are added:\n",
    "\n",
    "- LeakyRELU, function is h(x) = x if x ≥ 0, else h(x) = αx\n",
    "- ELU (exponential linear unit). The function is h(x) = x if x ≥ 0, else h(x) = α(exp(x)-1).\n",
    "- ReLU (rectified linear unit). THe function is h(x) = max(x, 0). The gradient of the function is zero in the region where x is negative, and the neuron is not active.\n",
    "\n",
    "There is many more types of activation functions that can be added to a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 120\n",
    "model = Sequential()\n",
    "\n",
    "# Initial layer\n",
    "# Adding LeakyRELU activation function\n",
    "model.add(Dense(nodes, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU())\n",
    "# randomly deactivates 25% of nodes during training\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(nodes, activation='elu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(nodes, activation='relu'))\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "model.add(Dense(nodes, activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(custom_activation))\n",
    "          \n",
    "# Compiles using mean squared error as loss function\n",
    "# rmspropr optimizer, adapts learning rate during training based on magnitude of recent gradients\n",
    "model.compile(loss='mse',optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the neural network model\n",
    "# Model is learning from training dataset X_train as input and y_train as target output\n",
    "# Data is split into mini-batches of 64 samples, updating node wirghts after processing each batch\n",
    "# Will iterate over entire training dataset 20 times\n",
    "# 10% of training data is held out as validation data for each epoch\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=20, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides summary of neural network model's architecure:\n",
    "    # Layer type and details, output shape parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from sklearn.metrics import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assess the accuracy of predictions generated by the model, comparing them to actual values\n",
    "# y is the actual value and y_hat is the predicted\n",
    "def CheckAccuracy(y,y_hat):\n",
    "    stats = dict()\n",
    "\n",
    "    stats['diff'] = y - y_hat\n",
    "    \n",
    "    #figure(figsize=(14,10))\n",
    "    plt.hist(stats['diff'], bins=50,edgecolor='black',color='white')\n",
    "    plt.xlabel('Diff')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values from the training set\n",
    "y_train_hat = model.predict(X_train)\n",
    "#reduce dim (240000,1) -> (240000,) to match y_train's dim\n",
    "y_train_hat = np.squeeze(y_train_hat)\n",
    "CheckAccuracy(y_train, y_train_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
