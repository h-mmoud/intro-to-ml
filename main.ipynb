{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Workshop Demo**: Introduction to Machine Learning in Finance\n",
    "\n",
    "- **Objectives**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Price Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hassan TODO\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Risk Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nico TODO\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deep Learning in Option Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Deep Learning**\n",
    "\n",
    "- [Deep Learning](https://www.ibm.com/topics/deep-learning) is a subset of machine learning that uses multilayered neural networks to simulate the complex decision-making power of the human brain.\n",
    "\n",
    "- Main difference between deep learning and machine learning is the structure of the underlying neural network architecture. \"Nondeep\" traditional ML models use simple neural networks with one or two computational layers. Deep learning models use neural networks with one or two compuetational layers. Deep learning models use three or more layers (typically hundreds or thousands of layers) to train the models.\n",
    "\n",
    "- Consists of multiple layers of interconnected notes, each building on previous to refine and optimize the prediction -> forward propogation. The `IN` and `OUT` layers of a deep neural network are called **visible** layers\n",
    "\n",
    "- Another process, backward propagation, is used to calculate errors in predictions and then adjust the weights and biases of between the node layers accordingly. The common approach to do this is via `gradient descent`.\n",
    "\n",
    "- Common deep learning neural networks: \n",
    "    - **CNNs** (convolution neural networks): computer vision and image classification applications.\n",
    "    - **RNNs** (recurrent neural networks): natural language and speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Option Pricing**\n",
    "- Formal definition of an [option](https://corporatefinanceinstitute.com/resources/derivatives/option-pricing-models/) states that it is a type of contract between two parties that provides one party the right, but **not** the oblidation, to buy or sell the underlying asset at a predetermined price before or at expirations day.\n",
    "\n",
    "    - Call option = buying\n",
    "    - Put option = selling\n",
    "\n",
    "- Buying or selling an option comes with a price called the option's premium. Buyer of an option pay the premium and sellers receive the premium\n",
    "\n",
    "- Factors determining the value of an option:\n",
    "\n",
    "    - Current stock price\n",
    "    - Intrinsic value\n",
    "    - Time to expiration\n",
    "    - Volatility\n",
    "    - Interest rates\n",
    "    - Cash dividends paid\n",
    "\n",
    "(ie. price of a stock rises -> likely price of call option will rise and the price of a put option will fall, vice versa)\n",
    "\n",
    "- Example pricing models: \n",
    "\n",
    "    - Binomial Model\n",
    "    - Trinomial Model\n",
    "    - Black-Scholes Model\n",
    "    - Monte-Carlo Simulation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Black-Scholes (1973)**\n",
    "\n",
    "- One of the most commonly used formula to price call and put options, there are many variations of this formula.\n",
    "- Model was first discovered in 1973 by economists Fischer black and Myron Scholes, developed mainly for pricing European options on stocks.\n",
    "\n",
    "- Some assumptions:\n",
    "    - Stock Price Distribution : continuously compounded returns on the stock are normally distributed and independent over time, volatility of continuously compounded returns is known and constant, future dividends are known (as a dollar amount/fixed dividend yield)\n",
    "    - Economic Environment : risk-free rate is known and constant, no transaction costs or taxes, possible to short-sell with no cost and to borrow at the risk-free rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example Black-Scholes Code**\n",
    "\n",
    "- Code below is from [Sanskar](https://github.com/Sanskar02/OPTION_PRICING_MODEL)\n",
    "- Another example model can be found [here](https://srdas.github.io/Papers/BlackScholesNN.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Random Values to Simulate a Stock Option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random Gaussian values\n",
    "from random import seed\n",
    "from random import gauss\n",
    "\n",
    "# seed random number generator\n",
    "seed(1)\n",
    "\n",
    "\n",
    "# generting Stock Price(S) vector\n",
    "list1 =[]\n",
    "# Generates 10000 simulated stock prices\n",
    "for i in range(10000):\n",
    "    # Stock prices assumed to drawn from normal distribution with mean of 250 and standard deviation of 50.11\n",
    "    k = gauss(250,50.11)\n",
    "    # Rounds up generated value to two decimal places\n",
    "    list1.append(round(k,2))\n",
    "    \n",
    "\n",
    "# generating Strike Price(K) vector\n",
    "list2 =[]\n",
    "for i in range(10000):\n",
    "    k = 0\n",
    "    # Generate non-negative stricke prices with mean of 322.6 and standard deviation of 65.9\n",
    "    k = abs(gauss(322.6,65.9))\n",
    "    while round(k,2) >= list1[i]:\n",
    "        k = abs(gauss(322.6,65.9))\n",
    "    list2.append(round(k,2))\n",
    "    \n",
    "\n",
    "# generate maturity time vector\n",
    "list3 =[]\n",
    "for i in range(10000):\n",
    "    k = gauss(541.4,111)\n",
    "    # Rounds generated values to 4 decimal places and converts value from days to years\n",
    "    list3.append(round(int(round(k,2))/365,4))\n",
    "    \n",
    "\n",
    "# generate dividend gain vector    \n",
    "list4 =[]\n",
    "for i in range(10000):\n",
    "    k = gauss(1.5,0.31)\n",
    "    list4.append(round(round(k,2)/100,6))\n",
    "    \n",
    "    \n",
    "# generate risk free interst rate vector\n",
    "list5 =[]\n",
    "for i in range(10000):\n",
    "    k = gauss(2.05,0.2)\n",
    "    list5.append(round(round(k,2)/100,6))\n",
    "\n",
    "\n",
    "# generate volatality vector\n",
    "list6 =[]\n",
    "for i in range(10000):\n",
    "    k = gauss(30,10)\n",
    "    list6.append(round(round(k,2)/100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listOfTuples(l1, l2,l3,l4,l5,l6): \n",
    "    return list(map(lambda x, y,z,w,i,o:(x,y,z,w,i,o), l1, l2,l3,l4,l5,l6)) \n",
    "\n",
    "p = listOfTuples(list1,list2,list3,list4,list5,list6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(p, columns=[\"S\",\"K\",\"T\",\"q\",\"r\",\"sigma\"])\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as si\n",
    "import sympy as sy\n",
    "from sympy.stats import Normal, cdf\n",
    "from sympy import init_printing\n",
    "init_printing()\n",
    "\n",
    "# Function to define the Black-Scholes model\n",
    "def black_scholes_call_div(S, K, T, r, q, sigma):\n",
    "    \n",
    "    #S: spot price (Price of underlying asset)\n",
    "    #K: strike price\n",
    "    #T: time to maturity\n",
    "    #r: interest rate\n",
    "    #q: rate of continuous dividend paying asset \n",
    "    #sigma: volatility\n",
    "    \n",
    "    # Two intermediate variables used in Black-Scholes formula\n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = (np.log(S / K) + (r - q - 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    \n",
    "   \n",
    "    call =  (S * np.exp(-q * T) * si.norm.cdf(d1, 0.0, 1.0) - K * np.exp(-r * T) * si.norm.cdf(d2, 0.0, 1.0))\n",
    "    \n",
    "    # Returning calculated call option prices\n",
    "    return round(call,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applting the Black-Scholes function to the randomly generated dataset\n",
    "df['C'] = df.apply(lambda row : black_scholes_call_div(row['S'], row['K'], row['T'],row['r'],row['q'],row['sigma']),axis = 1)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the DataFrame for efficient reload\n",
    "df.to_pickle(\"option_dataset\")\n",
    "\n",
    "# Creating new DataFrame object df1 that points to same objects as df\n",
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the roperty of linear homogeneity in the Black-Scholes model\n",
    "# Model's output price scales linearly with scale of input\n",
    "df1[\"S\"] = df1[\"S\"]/df1[\"K\"]\n",
    "df1[\"C\"] = df1[\"C\"]/df1[\"K\"]\n",
    "\n",
    "# Now database S represents ratio of spot price to strike price (S/K)\n",
    "# C represents ratio of normalizes option price (C/K)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing column K as it is now implied in the dataset under columns S and C\n",
    "df1.drop(columns = ['K'], inplace = True)\n",
    "df1.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.copy()\n",
    "# Dropping columns C and sigma (option price and volatility) to initialse training dataset\n",
    "X.drop(columns = ['C'], inplace = True)\n",
    "X.drop(columns = ['sigma'], inplace = True)\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining y as normalized C (option price) from before for future reference\n",
    "y = df1['C']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for creation of synthetic datasets and for splitting arrays/DataFrames into random train and test subsets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splits feature matrix X and target vector y into training and testing sets\n",
    "# 80% of data will be used for training and remaining 20% will be used for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state = 1)\n",
    "\n",
    "# Now stores 80% of X dataframe\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stores 80% of y dataframe, ground truth labels that the model will learn to predict from data in X_train\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TensorFlow](https://www.tensorflow.org/) framework is used for creating and deploying ML models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing TensorFlow library to prepare custom activation functions\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Defining a custom activation function\n",
    "def custom_activation(x):\n",
    "    return K.exp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the ANN(artificial neural network) Model**\n",
    "\n",
    "Three activation layers are added:\n",
    "\n",
    "- LeakyRELU, function is h(x) = x if x ≥ 0, else h(x) = αx\n",
    "- ELU (exponential linear unit). The function is h(x) = x if x ≥ 0, else h(x) = α(exp(x)-1).\n",
    "- ReLU (rectified linear unit). THe function is h(x) = max(x, 0). The gradient of the function is zero in the region where x is negative, and the neuron is not active.\n",
    "\n",
    "There is many more types of activation functions that can be added to a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = 120\n",
    "model = Sequential()\n",
    "\n",
    "# Initial layer\n",
    "# Adding LeakyRELU activation function\n",
    "model.add(Dense(nodes, input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU())\n",
    "# randomly deactivates 25% of nodes during training\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(nodes, activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nodes, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(nodes, activation='elu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(custom_activation))\n",
    "          \n",
    "# Compiles using mean squared error as loss function\n",
    "# rmspropr optimizer, adapts learning rate during training based on magnitude of recent gradients\n",
    "model.compile(loss='mse',optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the neural network model\n",
    "# Model is learning from training dataset X_train as input and y_train as target output\n",
    "# Data is split into mini-batches of 64 samples, updating node wirghts after processing each batch\n",
    "# Will iterate over entire training dataset 20 times\n",
    "# 10% of training data is held out as validation data for each epoch\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=20, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provides summary of neural network model's architecure:\n",
    "    # Layer type and details, output shape parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from sklearn.metrics import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assess the accuracy of predictions generated by the model, comparing them to actual values\n",
    "# y is the actual value and y_hat is the predicted\n",
    "def CheckAccuracy(y,y_hat):\n",
    "    stats = dict()\n",
    "\n",
    "    stats['diff'] = y - y_hat\n",
    "    \n",
    "    \n",
    "    #plot histogram to show the distribution of the differences between actual and predicted\n",
    "    mpl.rcParams['agg.path.chunksize'] = 100000\n",
    "    #figure(figsize=(14,10))\n",
    "    plt.scatter(y, y_hat,color='black',linewidth=0.3,alpha=0.4, s=0.5)\n",
    "    plt.xlabel('Actual Price',fontsize=20,fontname='Times New Roman')\n",
    "    plt.ylabel('Predicted Price',fontsize=20,fontname='Times New Roman') \n",
    "    plt.show()\n",
    "    \n",
    "    #figure(figsize=(14,10))\n",
    "    plt.hist(stats['diff'], bins=50,edgecolor='black',color='white')\n",
    "    plt.xlabel('Diff')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values from the training set\n",
    "y_train_hat = model.predict(X_train)\n",
    "#reduce dim (240000,1) -> (240000,) to match y_train's dim\n",
    "y_train_hat = np.squeeze(y_train_hat)\n",
    "CheckAccuracy(y_train, y_train_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
